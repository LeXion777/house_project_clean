from flask import Blueprint, render_template, request, session
from .llama_model import generate_chat

bp = Blueprint("llama", __name__)

# ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
DEFAULT_PARAMS = {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "max_tokens": 512,
    "repetition_penalty": 1.1,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0
}

# ê¸°ë³¸ System Prompt
DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."


@bp.route("/llama", methods=["GET", "POST"])
def llama_chat():
    # =========================
    # ì„¸ì…˜ ì´ˆê¸°í™”
    # =========================
    if "chat_history" not in session:
        session["chat_history"] = []

    if "params" not in session:
        session["params"] = DEFAULT_PARAMS.copy()

    if "system_prompt" not in session:
        session["system_prompt"] = DEFAULT_SYSTEM_PROMPT

    # =========================
    # POST ì²˜ë¦¬
    # =========================
    if request.method == "POST":
        action = request.form.get("action")

        # -------------------------
        # 1ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° / ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
        # -------------------------
        if action == "apply_params":
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
            for key in session["params"]:
                if key in request.form:
                    value = request.form[key]
                    session["params"][key] = (
                        float(value) if "." in value else int(value)
                    )

            # System Prompt ì €ì¥
            if "system_prompt" in request.form:
                session["system_prompt"] = request.form["system_prompt"]

        # -------------------------
        # 2ï¸âƒ£ ë©”ì‹œì§€ ì „ì†¡
        # -------------------------
        elif action == "send_message":
            user_input = request.form.get("prompt", "").strip()

            if user_input:
                session["chat_history"].append({
                    "role": "user",
                    "content": user_input
                })

                assistant_reply = generate_chat(
                    session["chat_history"],
                    system_prompt=session["system_prompt"],  # ğŸ”¥ í•µì‹¬
                    **session["params"]
                )

                session["chat_history"].append({
                    "role": "assistant",
                    "content": assistant_reply
                })

        session.modified = True

    # =========================
    # ë Œë”ë§
    # =========================
    return render_template(
        "llama/llama.html",
        chat_history=session["chat_history"],
        params=session["params"],
        system_prompt=session["system_prompt"]
    )
